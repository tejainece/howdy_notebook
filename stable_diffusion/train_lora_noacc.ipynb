{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b15113b-35ff-4ce3-ab4f-d8e1e10958ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "import xformers\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from torchvision import transforms\n",
    "from diffusers.optimization import get_scheduler\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf44b40-49e9-422f-8ea3-00183b87036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='runwayml/stable-diffusion-v1-5'\n",
    "train_data_dir='~/Pictures/lora_datasets/crystal'\n",
    "output_dir='/home/tejag/Pictures/lora_outputs/crystal'\n",
    "rank=4\n",
    "optimizer_cls = torch.optim.AdamW\n",
    "# Initial learning rate (after the potential warmup period) to use\n",
    "learning_rate=1e-4\n",
    "adam_beta1=0.9\n",
    "adam_beta2=0.999\n",
    "adam_weight_decay=1e-2\n",
    "adam_epsilon=1e-08\n",
    "image_column_name='image'\n",
    "text_column_name='text'\n",
    "resolution=512\n",
    "center_crop=True\n",
    "random_flip=True\n",
    "prediction_type=None\n",
    "max_grad_norm=1.0\n",
    "lr_scheduler='constant' #[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
    "lr_warmup_steps=500\n",
    "# TODO scaling LR\n",
    "\n",
    "# TODO\n",
    "num_train_epochs=50\n",
    "# Batch size (per device) for the training dataloader\n",
    "train_batch_size=1\n",
    "# Number of updates steps to accumulate before performing a backward/update pass\n",
    "gradient_accumulation_steps=1\n",
    "\n",
    "device='cuda'\n",
    "weight_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab3e538-af16-4ee5-b361-f0b168469e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\").to(device, dtype=weight_dtype)\n",
    "vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\").to(device, dtype=weight_dtype)\n",
    "unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\").to(device, dtype=weight_dtype)\n",
    "\n",
    "# freeze parameters of models to save more memory\n",
    "unet.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    unet.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be4242a1-2ecc-4636-8d8c-8125aa1e6751",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_attn_procs = {}\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "    lora_attn_procs[name] = LoRAAttnProcessor(\n",
    "        hidden_size=hidden_size,\n",
    "        cross_attention_dim=cross_attention_dim,\n",
    "        rank=rank,\n",
    "    ).to(device=device)\n",
    "\n",
    "unet.set_attn_processor(lora_attn_procs)\n",
    "lora_layers = AttnProcsLayers(unet.attn_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6747f866-cab5-4797-93c2-d36d90336748",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer_cls(\n",
    "    lora_layers.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(adam_beta1, adam_beta2),\n",
    "    weight_decay=adam_weight_decay,\n",
    "    eps=adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65fe0c2-0663-4761-a070-8de2303a44b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8bf74f2e544358811a3006d8eed1e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5f1b4b145e44d0ada024c36f3dff07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0d5c9b85074e45953f75706f8d9ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901c1c331d2c4b02bfb1d1301b9a0e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23fa9abed60447fb2bd57db39b27001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=train_data_dir,\n",
    "    #cache_dir=args.cache_dir,\n",
    ")\n",
    "#display(dataset)\n",
    "column_names = dataset[\"train\"].column_names\n",
    "#print(column_names)\n",
    "if image_column_name not in column_names:\n",
    "    raise f\"Image column {image_column_name} not in {','.join(column_names)}\"\n",
    "if text_column_name not in column_names:\n",
    "    raise f\"Text column {text_column_name} not in {','.join(column_names)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1738e647-8fcf-4f9f-87c6-6fb1833ff66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(args.resolution),\n",
    "    transforms.RandomHorizontalFlip() if random_flip else transforms.Lambda(lambda x: x),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples, is_train=True):\n",
    "    inputs = tokenizer(\n",
    "        examples[text_column_name], max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids.to(device=device)\n",
    "\n",
    "def preprocess_train(items):\n",
    "    # display(items)\n",
    "    # print('here')\n",
    "    images = [image.convert(\"RGB\") for image in items[image_column_name]]\n",
    "    items[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    items[\"input_ids\"] = tokenize_captions(items)\n",
    "    return items\n",
    "\n",
    "train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "def collate_fn(items):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in items])\n",
    "    pixel_values = pixel_values.to(device=device, memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in items])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=train_batch_size,\n",
    "    # num_workers=args.dataloader_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba4d465-a169-4e3a-8bc2-4842a93091a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps to perform.  If provided, overrides num_train_epochs.\n",
    "max_train_steps=num_train_epochs * len(train_dataloader) # TODO \n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0f82f2-346d-47e5-97be-cc905d21dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_epoch=0\n",
    "global_step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a6b2fd0-da4e-40ef-a11a-edd770fd2a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=49 Step=20\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(first_epoch, num_train_epochs):\n",
    "    unet.train()\n",
    "    train_loss=0.0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 10 == 0:\n",
    "            clear_output()\n",
    "            print(f'Epoch={epoch} Step={step}')\n",
    "        # TODO skip steps until we reach the resumed step\n",
    "\n",
    "        # TODO accelerate.accumulate\n",
    "\n",
    "        # Convert images to latent space\n",
    "        latents = vae.encode(batch[\"pixel_values\"].to(device=device,dtype=weight_dtype)).latent_dist.sample().to(device=device)\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "\n",
    "        bsz = latents.shape[0]\n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps).to(device=device)\n",
    "        \n",
    "        # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0].to(device=device)\n",
    "\n",
    "        \n",
    "        # Get the target for loss depending on the prediction type\n",
    "        if prediction_type is not None:\n",
    "            # set prediction_type of scheduler if defined\n",
    "            noise_scheduler.register_to_config(prediction_type=prediction_type)\n",
    "\n",
    "        if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "            target = noise\n",
    "        elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "            target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "        # Predict the noise residual and compute loss\n",
    "        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(lora_layers.parameters(), max_norm=max_grad_norm)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f6a183a-a886-4f5a-8b5f-4bff171c8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet.to(torch.float32)\n",
    "unet.save_attn_procs(output_dir, weight_name='tejag_crystal_1.safetensors', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a270abc-5a86-4b29-b437-1f8cc31e11f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
