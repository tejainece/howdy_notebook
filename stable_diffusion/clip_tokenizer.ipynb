{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebff9b4-96aa-44c6-8952-31ec28a5656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "logging.disable(logging.WARNING)  \n",
    "from transformers import CLIPTextModel, CLIPTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135f5909-a29a-4e94-ba62-a335df677ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91dbb89df284ee8bccd2aa0d7798327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO use stable diffusion safetensor file\n",
    "# openai/clip-vit-large-patch14\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c66336-ce95-4b9f-82de-5038c575edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words shape: torch.Size([1, 77])\n",
      "Attention shape: torch.Size([1, 77])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406,   320,  1929,  3309,  3801, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = [\"a dog wearing hat\"]\n",
    "tok = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") \n",
    "print(f'Words shape: {tok.input_ids.shape}')\n",
    "print(f'Attention shape: {tok.attention_mask.shape}')\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3db72c0f-7270-41ff-9e2d-f32bb802a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49406:<|startoftext|>\n",
      "320:a</w>\n",
      "1929:dog</w>\n",
      "3309:wearing</w>\n",
      "3801:hat</w>\n",
      "49407:<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "for token in tok.input_ids[0,:6]:\n",
    "    print(f\"{token}:{tokenizer.convert_ids_to_tokens(token.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ade4be29-e212-4db3-90d1-9ad1f65ae215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding : torch.Size([1, 77, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4902, -0.3066,  0.0674],\n",
       "         [ 0.0278, -1.3262,  0.3086,  ..., -0.5239,  0.9731,  0.6660],\n",
       "         [-1.5928,  0.5059,  1.0801,  ..., -1.5273, -0.8433,  0.1587],\n",
       "         ...,\n",
       "         [-1.4707,  0.3135,  1.1670,  ...,  0.3743,  0.5376, -1.5039],\n",
       "         [-1.4697,  0.3022,  1.1777,  ...,  0.3752,  0.5420, -1.5000],\n",
       "         [-1.4424,  0.3142,  1.1982,  ...,  0.3516,  0.5420, -1.5498]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = text_encoder(tok.input_ids.to(\"cuda\"))[0].half()\n",
    "print(f\"Shape of embedding : {emb.shape}\")\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039786b-6efc-40cc-b23c-83dfaa6db6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
